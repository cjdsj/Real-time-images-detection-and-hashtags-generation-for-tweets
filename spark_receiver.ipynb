{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "feae48b0-6ce1-4366-a3dd-3bb46219a711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Arrow support.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"64\")\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "from typing import Iterator, Tuple\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch import Tensor\n",
    " \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets.folder import default_loader  # private API\n",
    " \n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4ff4ea6-36e1-4b05-8aee-87e31fb09b2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_path = self.paths[item]\n",
    "        \n",
    "        response = requests.get(image_path)\n",
    "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "            \n",
    "\n",
    "def get_model_for_eval():\n",
    "    \"\"\"Gets the broadcasted model.\"\"\"\n",
    "    model = load_model()\n",
    "    model.load_state_dict(bc_model_state.value)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# @pandas_udf(ArrayType(FloatType()))\n",
    "def predict_batch(paths):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "      ])\n",
    "    images = ImageDataset(paths, transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(images, batch_size=32, num_workers=8)\n",
    "    model = get_model_for_eval()\n",
    "#     model.to(device)\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            predictions = list(model(batch).cpu().numpy())\n",
    "            for prediction in predictions:\n",
    "                all_predictions.append(prediction)\n",
    "    return pd.Series(all_predictions)\n",
    "\n",
    "def load_model(model_path=None):\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 13)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "    \n",
    "    \n",
    "bc_model_state = sc.broadcast(load_model('/dbfs/FileStore/model.h5').state_dict())\n",
    "predict_udf = pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR)(predict_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cfd9edba-78d7-4636-ae16-6be882bf6c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import sys\n",
    "import requests\n",
    "import copy\n",
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "# create spark context with the above configuration\n",
    "# sc = SparkContext(conf=conf)\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "# create the Streaming Context from the above spark context with interval size 2 seconds\n",
    "ssc = StreamingContext(sc, 8)\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "# ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "# read data from port 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\",9017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb85fbb0-783f-4c96-806f-fd972dd6988c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_rdd(time, rdd):\n",
    "    # time rdd arrived\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        # rdd operations here\n",
    "#         print(rdd.collect())\n",
    "        if not rdd.isEmpty():\n",
    "            df = rdd.map(lambda x: (x, )).toDF(['url'])\n",
    "            predictions_df = df.select(col('url'), predict_udf(col('url')).alias(\"prediction\"))\n",
    "            res = predictions_df.take(4)\n",
    "#             print(res)\n",
    "            np.save(f\"/dbfs/FileStore/animals/{str(time)}.npy\", np.array(res))\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a663fd92-4bb0-4288-b750-364d621ea8e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# functions to be implemented\n",
    "dataStream.foreachRDD(process_rdd)\n",
    "# dataStream.pprint()\n",
    "\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "\n",
    "# wait for the streaming to finish, timeout can be removed to continuely streaming\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_receiver",
   "notebookOrigID": 453430745507806,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
